{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_corpus = json.load(open('clean data/pre_corpus.json'))\n",
    "post_corpus = json.load(open('clean data/post_corpus.json'))                \n",
    "\n",
    "combined_corpus = pre_corpus + post_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12334"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Get a list of article body string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [art['body'] for art in combined_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\2025m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "english_stop_words = stopwords.words('english')\n",
    "english_stop_words.extend(['vaccine', 'vaccination', 'vaccines'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI/ML: Topic modeling on NYT articles\n",
    "\n",
    "* Topic Modeling is an unsupervised machine learning technique that looks for a specified number of __TOPICS__ presumed to be the possible meaning space behind a set of documents (i.e., _in this context these are the N things people are likely to talk about_). In an iterative procedure patterns of term co-occurrence (topics) are found across documents. \n",
    "\n",
    "* The result is that for each document each of the N topics has a probability distribution (summing to 1) indicating the likelihood or contribution of each topic to the document. Topics are rankings of terms.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Build a DTM from the `body` text field\n",
    "2. Set up a LDA model with a specified number of topics to discover\n",
    "3. Run the modeling\n",
    "4. Examine the topics by looking at the top ranked terms in each topic and evaluating the plausibility of them as semantic groups.\n",
    "5. Use document topic distributions in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1 Define DTM\n",
    "article_cnt_vect = CountVectorizer(\n",
    "    token_pattern=\"[a-z][a-z'-]+\",\n",
    "    stop_words=english_stop_words,\n",
    "    max_df=0.7,\n",
    "    min_df=10,\n",
    "    max_features=5000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "art_dtm = article_cnt_vect.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12334x5000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3972875 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "art_dtm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DTM is 12,334 documents and 5,000 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2\n",
    "N_topics = 7\n",
    "article_LDA = LatentDirichletAllocation(n_components=N_topics,\n",
    "                                        max_iter=10,\n",
    "                                        learning_method='online',\n",
    "                                        learning_offset=1,\n",
    "                                        batch_size=256,\n",
    "                                        random_state=0,\n",
    "                                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1 of max_iter: 10\n",
      "iteration: 2 of max_iter: 10\n",
      "iteration: 3 of max_iter: 10\n",
      "iteration: 4 of max_iter: 10\n",
      "iteration: 5 of max_iter: 10\n",
      "iteration: 6 of max_iter: 10\n",
      "iteration: 7 of max_iter: 10\n",
      "iteration: 8 of max_iter: 10\n",
      "iteration: 9 of max_iter: 10\n",
      "iteration: 10 of max_iter: 10\n"
     ]
    }
   ],
   "source": [
    "tm = article_LDA.fit_transform(art_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_,1):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "====================\n",
      "\n",
      "Topic #1: mr trump president biden times house like white york political time republican news election first even administration party could campaign state former country republicans democrats national us ms think years many government day two get american back last americans coronavirus\n",
      "\n",
      "Topic #2: company percent companies million billion year government could federal economy economic mr pandemic market business coronavirus drug states money last administration pfizer doses industry financial week businesses united help chief stock moderna next executive even spending unemployment congress prices may\n",
      "\n",
      "Topic #3: care dr public medical children state school department federal like get york many patients hospital program parents doctors university need ms say work make could system government services years hospitals time mr law center workers insurance child director www think\n",
      "\n",
      "Topic #4: virus dr disease flu coronavirus cases outbreak officials infected spread could experts states pandemic may public covid- testing control united ebola fauci two like infectious scientists centers response first even get patients symptoms many influenza infection tests prevention human year\n",
      "\n",
      "Topic #5: city york coronavirus state cases virus pandemic many officials states country week home day times percent first two public last get schools school may back year weeks restrictions workers days like reported covid- even county number travel residents students deaths\n",
      "\n",
      "Topic #6: dr aids research disease drug study scientists percent women years researchers trials drugs many cancer virus patients children could trial may blood studies university year two diseases immune treatment cells first use human institute among found medical national clinical even\n",
      "\n",
      "Topic #7: united world countries government china states officials american international nations country million military chinese global last organization war two mr africa years minister security european year many could polio smallpox first foreign since children india iraq south britain national europe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_top_words=40\n",
    "print(\"\\nTopics in LDA model:\\n====================\\n\")\n",
    "tf_feature_names = article_cnt_vect.get_feature_names_out()\n",
    "print_top_words(article_LDA, tf_feature_names, n_top_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of topic model\n",
    "\n",
    "* The `pyLDAvis` module provides a nice interactive visualization of a topic model.\n",
    "\n",
    "* Each topic is represented by a circle, the size of which indicates the proportion of topic representation in the corpus. The positioning in 2D space of the topic circles gives an indication of how distinct the topics are (often see some level of overlap because of shared terms).\n",
    "\n",
    "* The bar graph on the right indicates the top 30 terms most associated with a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LatentDirichletAllocation' object has no attribute 'num_topics'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m dictionary \u001b[39m=\u001b[39m corpora\u001b[39m.\u001b[39mDictionary([feature_names])\n\u001b[0;32m     17\u001b[0m \u001b[39m# Now you can use the dictionary with pyLDAvis\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m pviz \u001b[39m=\u001b[39m gensimvis\u001b[39m.\u001b[39;49mprepare(article_LDA, art_dtm, dictionary, sort_topics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\2025m\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m     \u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39mprepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\2025m\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:42\u001b[0m, in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     40\u001b[0m     num_topics \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(topic_model\u001b[39m.\u001b[39mlda_alpha)\n\u001b[0;32m     41\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     num_topics \u001b[39m=\u001b[39m topic_model\u001b[39m.\u001b[39;49mnum_topics\n\u001b[0;32m     44\u001b[0m \u001b[39mif\u001b[39;00m doc_topic_dists \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     45\u001b[0m     \u001b[39m# If its an HDP model.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(topic_model, \u001b[39m'\u001b[39m\u001b[39mlda_beta\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LatentDirichletAllocation' object has no attribute 'num_topics'"
     ]
    }
   ],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import corpora\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Convert CountVectorizer to gensim Dictionary\n",
    "corpus = ['This is the first document.', 'This document is the second document.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the vocabulary from CountVectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the vocabulary to gensim Dictionary\n",
    "dictionary = corpora.Dictionary([feature_names])\n",
    "\n",
    "# Now you can use the dictionary with pyLDAvis\n",
    "pviz = gensimvis.prepare(article_LDA, art_dtm, dictionary, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'token2id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pviz\u001b[39m=\u001b[39mgensimvis\u001b[39m.\u001b[39;49mprepare(article_LDA, art_dtm, article_cnt_vect, sort_topics\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\2025m\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:122\u001b[0m, in \u001b[0;36mprepare\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dist, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare\u001b[39m(topic_model, corpus, dictionary, doc_topic_dist\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     78\u001b[0m     \u001b[39m\"\"\"Transforms the Gensim TopicModel and related corpus and dictionary into\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m    the data structures needed for the visualization.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39m    See `pyLDAvis.prepare` for **kwargs.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     opts \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mmerge(_extract_data(topic_model, corpus, dictionary, doc_topic_dist), kwargs)\n\u001b[0;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m pyLDAvis\u001b[39m.\u001b[39mprepare(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\2025m\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyLDAvis\\gensim_models.py:23\u001b[0m, in \u001b[0;36m_extract_data\u001b[1;34m(topic_model, corpus, dictionary, doc_topic_dists)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[39m# Need corpus to be a streaming gensim list corpus for len and inference functions below:\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     corpus \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmatutils\u001b[39m.\u001b[39mSparse2Corpus(corpus_csc)\n\u001b[1;32m---> 23\u001b[0m vocab \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(dictionary\u001b[39m.\u001b[39;49mtoken2id\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     24\u001b[0m \u001b[39m# TODO: add the hyperparam to smooth it out? no beta in online LDA impl.. hmm..\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m# for now, I'll just make sure we don't ever get zeros...\u001b[39;00m\n\u001b[0;32m     26\u001b[0m beta \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'token2id'"
     ]
    }
   ],
   "source": [
    "pviz=gensimvis.prepare(article_LDA, art_dtm, article_cnt_vect, sort_topics=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
