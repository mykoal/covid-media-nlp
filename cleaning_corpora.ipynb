{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from striprtf.striprtf import rtf_to_text\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "* Create functions to:\n",
    "    * Process file:\n",
    "        1. Load RTF file\n",
    "        2. Convert to `str` object\n",
    "        3. Split into a list of documents\n",
    "    * Process article/document\n",
    "        1. Split document into components\n",
    "        2. extracted needed fields, e.g. date, source, etc.\n",
    "        3. create a dictionary \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_RTF_file(fpath):\n",
    "    print(f'Processing {fpath}...')\n",
    "    # 1. load file\n",
    "    rtf = open(fpath).read()\n",
    "    \n",
    "    # 2. convert to plain text str\n",
    "    text = rtf_to_text(rtf, errors=\"ignore\")\n",
    "    \n",
    "    # 3. split into a list of documents\n",
    "    docs = text.split('End of Document')\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_doc(doc):\n",
    "    \n",
    "    date_RE = re.compile(r'^[JFMASOND][a-z]+ \\d{1,2}, \\d{4}')\n",
    "    \n",
    "    doc = doc.strip().replace('\\xa0',' ')\n",
    "    doc_dict = {}\n",
    "\n",
    "    # 1. find body indices\n",
    "    body_start = doc.index('\\nBody')+5\n",
    "    \n",
    "    try:\n",
    "        body_end = doc.index('Load-Date:')\n",
    "    except:\n",
    "        try:\n",
    "            body_end = doc.index('Correction-Date:')\n",
    "        except:\n",
    "            body_end = len(doc)\n",
    "    \n",
    "    doc_dict['body'] = re.sub(' +',' ',doc[body_start:body_end].strip())\n",
    "    doc_dict['footer'] = doc[body_end:]\n",
    "        \n",
    "    # 2. assume first three lines are:\n",
    "    #    TITLE\n",
    "    #    PUBLICATION\n",
    "    #    DATE\n",
    "    \n",
    "    header = doc[:body_start-4]\n",
    "    header_lines = header.split('\\n')\n",
    "    \n",
    "    if len(header_lines)<2:\n",
    "        print(header_lines, doc)\n",
    "    \n",
    "    # test if 3rd line has date pattern if not find index of line\n",
    "    dateline_idx = [idx for idx, line in enumerate(header_lines[2:],2) \n",
    "                                        if date_RE.match(line)]\n",
    "    \n",
    "    if len(dateline_idx):\n",
    "        date_idx = dateline_idx[0]\n",
    "        doc_dict['pubdate'] = header_lines[date_idx]\n",
    "    else:\n",
    "        date_idx=None\n",
    "        \n",
    "        if doc_dict['footer'].startswith('Correction-Date:'):\n",
    "            date_str=doc_dict['footer'].split('\\n')[0].split(':')[1]\n",
    "            date_str = f'{date_str.split(\"day, \")[0]}day'\n",
    "            doc_dict['pubdate'] = date_str\n",
    "            print(\"#### \", date_str)\n",
    "        elif doc_dict['footer'].startswith('Load-Date'):\n",
    "            date_str = doc_dict['footer'].split('\\n')[0].split(':')[1].strip()\n",
    "            print(\">>>> \", date_str)\n",
    "            doc_dict['pubdate'] = date_str\n",
    "        else:\n",
    "            doc_dict['pubdate'] = None\n",
    "    \n",
    "    if date_idx==3:\n",
    "        doc_dict['title']=header_lines[0]\n",
    "        doc_dict['subtitle']=header_lines[1]\n",
    "        doc_dict['publication']=header_lines[2]\n",
    "    else:\n",
    "        doc_dict['title']=header_lines[0]\n",
    "        doc_dict['publication']=header_lines[1]        \n",
    "\n",
    "    if doc_dict['pubdate'] and doc_dict['pubdate'].count('day,')>0:\n",
    "        pdate, edition = doc_dict['pubdate'].split('day, ')\n",
    "        doc_dict['pubdate']=f'{pdate}day'\n",
    "        doc_dict['edition']=edition.strip()\n",
    "    \n",
    "    for hline in header_lines:\n",
    "        lmatch = re.match('^([A-Za-z]+):\\s(.*)', hline)\n",
    "        if lmatch:\n",
    "            doc_dict[lmatch.group(1).lower()]=lmatch.group(2)\n",
    "    \n",
    "    return doc_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the RTF files in `raw data/post`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "postDIR = 'raw data/post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "postRTF_files = [f'{postDIR}/{fn}' for fn in os.listdir(postDIR) if fn.endswith('.rtf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(postRTF_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw data/post/files_after_1.rtf',\n",
       " 'raw data/post/files_after_101.rtf',\n",
       " 'raw data/post/files_after_1083.rtf',\n",
       " 'raw data/post/files_after_1183.rtf',\n",
       " 'raw data/post/files_after_1283.rtf',\n",
       " 'raw data/post/files_after_1383.rtf',\n",
       " 'raw data/post/files_after_1483.rtf',\n",
       " 'raw data/post/files_after_1583.rtf',\n",
       " 'raw data/post/files_after_1683.rtf',\n",
       " 'raw data/post/files_after_1783.rtf',\n",
       " 'raw data/post/files_after_1883.rtf',\n",
       " 'raw data/post/files_after_1973.rtf',\n",
       " 'raw data/post/files_after_201.rtf',\n",
       " 'raw data/post/files_after_2073.rtf',\n",
       " 'raw data/post/files_after_2173.rtf',\n",
       " 'raw data/post/files_after_2273.rtf',\n",
       " 'raw data/post/files_after_2373.rtf',\n",
       " 'raw data/post/files_after_2473.rtf',\n",
       " 'raw data/post/files_after_2573.rtf',\n",
       " 'raw data/post/files_after_2673.rtf',\n",
       " 'raw data/post/files_after_2773.rtf',\n",
       " 'raw data/post/files_after_2873.rtf',\n",
       " 'raw data/post/files_after_3000 (1).rtf',\n",
       " 'raw data/post/files_after_3000 (10).rtf',\n",
       " 'raw data/post/files_after_3000 (2).rtf',\n",
       " 'raw data/post/files_after_3000 (3).rtf',\n",
       " 'raw data/post/files_after_3000 (4).rtf',\n",
       " 'raw data/post/files_after_3000 (5).rtf',\n",
       " 'raw data/post/files_after_3000 (6).rtf',\n",
       " 'raw data/post/files_after_3000 (7).rtf',\n",
       " 'raw data/post/files_after_3000 (8).rtf',\n",
       " 'raw data/post/files_after_3000 (9).rtf',\n",
       " 'raw data/post/files_after_301.rtf',\n",
       " 'raw data/post/files_after_4000 (1).rtf',\n",
       " 'raw data/post/files_after_4000 (10).rtf',\n",
       " 'raw data/post/files_after_4000 (2).rtf',\n",
       " 'raw data/post/files_after_4000 (3).rtf',\n",
       " 'raw data/post/files_after_4000 (4).rtf',\n",
       " 'raw data/post/files_after_4000 (5).rtf',\n",
       " 'raw data/post/files_after_4000 (6) (1).rtf',\n",
       " 'raw data/post/files_after_4000 (6).rtf',\n",
       " 'raw data/post/files_after_4000 (7).rtf',\n",
       " 'raw data/post/files_after_4000 (8).rtf',\n",
       " 'raw data/post/files_after_4000 (9).rtf',\n",
       " 'raw data/post/files_after_401.rtf',\n",
       " 'raw data/post/files_after_501.rtf',\n",
       " 'raw data/post/files_after_601.rtf',\n",
       " 'raw data/post/files_after_701.rtf',\n",
       " 'raw data/post/files_after_801.rtf',\n",
       " 'raw data/post/files_after_901.rtf']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postRTF_files[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 get list of docs from RTF files\n",
    "# WARNING: THIS TAKES HOURS TO RUN ON YOUR OWN COMPUTER\n",
    "post_docs = []\n",
    "for rtf_file in postRTF_files:\n",
    "    post_docs.extend(process_RTF_file(rtf_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_post_docs = []\n",
    "for doc in post_docs:\n",
    "    if doc.strip()=='':\n",
    "        continue\n",
    "    all_post_docs.append(process_doc(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean data/post_corpus.json', 'w') as out:\n",
    "    out.write(json.dumps(all_docs, indent=4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the RTF files in `raw data/pre`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "preDIR = 'raw data/pre'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "preRTF_files = [f'{preDIR}/{fn}' for fn in os.listdir(preDIR) if fn.endswith('.rtf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preRTF_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw data/pre/files_before_0_1000 (1).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (10).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (2).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (3).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (4).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (5).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (6).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (7).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (8).rtf',\n",
       " 'raw data/pre/files_before_0_1000 (9).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (1).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (10).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (2).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (3).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (4).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (5).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (6).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (7).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (8).rtf',\n",
       " 'raw data/pre/files_before_1000_2000 (9).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (1).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (10).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (2).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (3).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (4).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (5).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (6).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (7).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (8).rtf',\n",
       " 'raw data/pre/files_before_2000_3000 (9).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (1).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (10).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (2).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (3).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (4).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (5).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (6).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (7).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (8).rtf',\n",
       " 'raw data/pre/files_before_3000_4000 (9).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (1).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (10).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (2).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (3).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (4).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (5).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (6).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (7).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (8).rtf',\n",
       " 'raw data/pre/files_before_4000_5000 (9).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (1).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (2).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (3).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (4).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (5).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (6).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (7).rtf',\n",
       " 'raw data/pre/files_before_5000_6000  (8).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (1).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (2).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (3).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (4).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (5).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (6).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (7).rtf',\n",
       " 'raw data/pre/files_before_6000_7000 (8).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (1).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (2).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (3).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (4).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (5).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (6).rtf',\n",
       " 'raw data/pre/files_before_7000_8000 (7).rtf']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preRTF_files[0:73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 get list of docs from RTF files\n",
    "# WARNING: THIS TAKES HOURS TO RUN ON YOUR OWN COMPUTER\n",
    "pre_docs = []\n",
    "for rtf_file in preRTF_files:\n",
    "    pre_docs.extend(process_RTF_file(rtf_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pre_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pre_docs = []\n",
    "for pre_doc in pre_docs:\n",
    "    if pre_doc.strip()=='':\n",
    "        continue\n",
    "    all_pre_docs.append(process_doc(pre_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clean data/pre_corpus.json', 'w') as out:\n",
    "    out.write(json.dumps(all_pre_docs, indent=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
